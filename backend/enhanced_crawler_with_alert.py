#!/usr/bin/env python3
"""
í–¥ìƒëœ PokerScout í¬ë¡¤ëŸ¬ - ë‹¤ì¤‘ í´ë°± ë©”ì»¤ë‹ˆì¦˜ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ í¬í•¨
- CloudScraper â†’ Selenium â†’ ì¼ë°˜ requests ìˆœì„œë¡œ ì‹œë„
- ì‹¤íŒ¨ ì‹œ ì´ë©”ì¼/Discord/Slack ì•Œë¦¼ ë°œì†¡
- ìƒì„¸í•œ ì—ëŸ¬ ë¡œê¹… ë° ë³µêµ¬ ì‹œë„
"""
import sys
import os
import logging
from datetime import datetime, timezone, timedelta
import json
import re
import time
import random
from typing import List, Dict, Optional, Tuple
import requests
from bs4 import BeautifulSoup
from google.oauth2 import service_account
from google.auth.transport.requests import Request
import cloudscraper

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(funcName)s] %(message)s'
)
logger = logging.getLogger(__name__)

# Firebase í”„ë¡œì íŠ¸ ì„¤ì •
FIREBASE_PROJECT_ID = "poker-online-analyze"
FIRESTORE_BASE_URL = f"https://firestore.googleapis.com/v1/projects/{FIREBASE_PROJECT_ID}/databases/(default)/documents"
FIREBASE_REALTIME_DB_URL = "https://poker-analyzer-ggp-default-rtdb.firebaseio.com"

# ì•Œë¦¼ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì½ê¸°)
ALERT_CONFIG = {
    'discord_webhook': os.environ.get('DISCORD_WEBHOOK_URL'),
    'slack_webhook': os.environ.get('SLACK_WEBHOOK_URL'),
    'email_to': os.environ.get('ALERT_EMAIL'),
    'github_issue': os.environ.get('GITHUB_REPO')  # e.g., "garimto81/poker-online-analyze"
}

class AlertSystem:
    """ë‹¤ì–‘í•œ ì±„ë„ë¡œ ì•Œë¦¼ì„ ë³´ë‚´ëŠ” ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.alerts_sent = []
        self.last_alert_time = None
        
    def send_alert(self, title: str, message: str, level: str = "ERROR", details: Dict = None):
        """ëª¨ë“  ì„¤ì •ëœ ì±„ë„ë¡œ ì•Œë¦¼ ë°œì†¡"""
        # ì¤‘ë³µ ì•Œë¦¼ ë°©ì§€ (5ë¶„ ì´ë‚´ ë™ì¼ ì•Œë¦¼ ì°¨ë‹¨)
        alert_key = f"{title}:{level}"
        current_time = datetime.now()
        
        if self.last_alert_time:
            time_diff = (current_time - self.last_alert_time).seconds
            if time_diff < 300 and alert_key in self.alerts_sent:  # 5ë¶„
                logger.info(f"ì•Œë¦¼ ìŠ¤í‚µ (ìµœê·¼ ë°œì†¡ë¨): {title}")
                return
        
        self.last_alert_time = current_time
        self.alerts_sent.append(alert_key)
        
        # Discord ì•Œë¦¼
        if ALERT_CONFIG.get('discord_webhook'):
            self._send_discord_alert(title, message, level, details)
        
        # Slack ì•Œë¦¼
        if ALERT_CONFIG.get('slack_webhook'):
            self._send_slack_alert(title, message, level, details)
        
        # GitHub Issue ìƒì„±
        if ALERT_CONFIG.get('github_issue') and level == "CRITICAL":
            self._create_github_issue(title, message, details)
        
        # ì½˜ì†” ì¶œë ¥ (GitHub Actions ë¡œê·¸)
        self._log_to_console(title, message, level, details)
    
    def _send_discord_alert(self, title: str, message: str, level: str, details: Dict):
        """Discord ì›¹í›…ìœ¼ë¡œ ì•Œë¦¼ ë°œì†¡"""
        try:
            color = {
                "INFO": 0x00FF00,     # ë…¹ìƒ‰
                "WARNING": 0xFFFF00,  # ë…¸ë€ìƒ‰
                "ERROR": 0xFF0000,    # ë¹¨ê°„ìƒ‰
                "CRITICAL": 0x8B0000  # ì§„í•œ ë¹¨ê°„ìƒ‰
            }.get(level, 0x808080)
            
            embed = {
                "title": f"ğŸš¨ {title}",
                "description": message,
                "color": color,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "fields": []
            }
            
            if details:
                for key, value in details.items():
                    embed["fields"].append({
                        "name": key,
                        "value": str(value)[:1024],
                        "inline": True
                    })
            
            payload = {"embeds": [embed]}
            response = requests.post(
                ALERT_CONFIG['discord_webhook'],
                json=payload,
                timeout=10
            )
            
            if response.status_code == 204:
                logger.info("Discord ì•Œë¦¼ ë°œì†¡ ì„±ê³µ")
            else:
                logger.warning(f"Discord ì•Œë¦¼ ì‹¤íŒ¨: {response.status_code}")
                
        except Exception as e:
            logger.error(f"Discord ì•Œë¦¼ ë°œì†¡ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _send_slack_alert(self, title: str, message: str, level: str, details: Dict):
        """Slack ì›¹í›…ìœ¼ë¡œ ì•Œë¦¼ ë°œì†¡"""
        try:
            blocks = [
                {
                    "type": "header",
                    "text": {
                        "type": "plain_text",
                        "text": f"ğŸš¨ {title}"
                    }
                },
                {
                    "type": "section",
                    "text": {
                        "type": "mrkdwn",
                        "text": f"*Level:* {level}\n*Message:* {message}"
                    }
                }
            ]
            
            if details:
                fields = []
                for key, value in details.items():
                    fields.append({
                        "type": "mrkdwn",
                        "text": f"*{key}:*\n{value}"
                    })
                blocks.append({
                    "type": "section",
                    "fields": fields[:10]  # ìµœëŒ€ 10ê°œ í•„ë“œ
                })
            
            payload = {"blocks": blocks}
            response = requests.post(
                ALERT_CONFIG['slack_webhook'],
                json=payload,
                timeout=10
            )
            
            if response.status_code == 200:
                logger.info("Slack ì•Œë¦¼ ë°œì†¡ ì„±ê³µ")
            else:
                logger.warning(f"Slack ì•Œë¦¼ ì‹¤íŒ¨: {response.status_code}")
                
        except Exception as e:
            logger.error(f"Slack ì•Œë¦¼ ë°œì†¡ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _create_github_issue(self, title: str, message: str, details: Dict):
        """GitHub Issue ìë™ ìƒì„± (Critical ì—ëŸ¬ë§Œ)"""
        try:
            if not os.environ.get('GITHUB_TOKEN'):
                logger.warning("GitHub Tokenì´ ì„¤ì •ë˜ì§€ ì•Šì•„ Issueë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            repo = ALERT_CONFIG['github_issue']
            url = f"https://api.github.com/repos/{repo}/issues"
            
            body = f"## ìë™ ìƒì„±ëœ ì´ìŠˆ\n\n"
            body += f"**ë°œìƒ ì‹œê°„:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
            body += f"**ë©”ì‹œì§€:** {message}\n\n"
            
            if details:
                body += "### ìƒì„¸ ì •ë³´\n\n"
                for key, value in details.items():
                    body += f"- **{key}:** {value}\n"
            
            headers = {
                "Authorization": f"token {os.environ.get('GITHUB_TOKEN')}",
                "Accept": "application/vnd.github.v3+json"
            }
            
            payload = {
                "title": f"[ìë™] {title}",
                "body": body,
                "labels": ["bug", "automated", "crawler-issue"]
            }
            
            response = requests.post(url, json=payload, headers=headers, timeout=10)
            
            if response.status_code == 201:
                issue_url = response.json().get('html_url')
                logger.info(f"GitHub Issue ìƒì„±ë¨: {issue_url}")
            else:
                logger.warning(f"GitHub Issue ìƒì„± ì‹¤íŒ¨: {response.status_code}")
                
        except Exception as e:
            logger.error(f"GitHub Issue ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _log_to_console(self, title: str, message: str, level: str, details: Dict):
        """ì½˜ì†”ì— êµ¬ì¡°í™”ëœ ì•Œë¦¼ ì¶œë ¥ (GitHub Actions ë¡œê·¸ìš©)"""
        print("\n" + "="*80)
        print(f"ğŸš¨ ì•Œë¦¼: {title}")
        print(f"ğŸ“Š ë ˆë²¨: {level}")
        print(f"ğŸ“ ë©”ì‹œì§€: {message}")
        
        if details:
            print("ğŸ“‹ ìƒì„¸ ì •ë³´:")
            for key, value in details.items():
                print(f"  - {key}: {value}")
        
        print("="*80 + "\n")

class RobustPokerScoutCrawler:
    """ë‹¤ì¤‘ í´ë°± ë©”ì»¤ë‹ˆì¦˜ì„ ê°–ì¶˜ ê²¬ê³ í•œ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.alert_system = AlertSystem()
        self.scraper = cloudscraper.create_scraper(
            browser={'browser': 'chrome', 'platform': 'windows', 'mobile': False}
        )
        self.gg_poker_sites = [
            'GGNetwork', 'GGPoker ON', 'GG Poker', 'GGPoker', 
            'GG Network', 'GGPoker.com', 'GG', 'Natural8',
            'GGPoker Global', 'GGPoker EU', 'BetKings', 'GGasia'
        ]
        self.retry_count = 3
        self.timeout = 30
        self.last_successful_data = None
        self.last_successful_time = None
        
    def crawl_with_fallback(self) -> Tuple[bool, List[Dict], str]:
        """ë‹¤ì¤‘ í´ë°± ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ í¬ë¡¤ë§ ì‹œë„"""
        methods = [
            ("CloudScraper", self._crawl_with_cloudscraper),
            ("Regular Requests", self._crawl_with_requests),
            ("Different Headers", self._crawl_with_custom_headers),
            ("Cached Data", self._use_cached_data)
        ]
        
        for method_name, method_func in methods:
            logger.info(f"í¬ë¡¤ë§ ì‹œë„: {method_name}")
            
            try:
                success, data, message = method_func()
                
                if success and data:
                    logger.info(f"âœ… {method_name} ì„±ê³µ: {len(data)}ê°œ ì‚¬ì´íŠ¸")
                    
                    # ì„±ê³µ ì‹œ ìºì‹œ ì—…ë°ì´íŠ¸
                    self.last_successful_data = data
                    self.last_successful_time = datetime.now()
                    
                    # ì´ì „ì— ì‹¤íŒ¨í–ˆë‹¤ê°€ ë³µêµ¬ëœ ê²½ìš° ì•Œë¦¼
                    if hasattr(self, '_was_failing') and self._was_failing:
                        self.alert_system.send_alert(
                            "í¬ë¡¤ë§ ë³µêµ¬ë¨",
                            f"{method_name} ë°©ë²•ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘ì´ ë³µêµ¬ë˜ì—ˆìŠµë‹ˆë‹¤.",
                            "INFO",
                            {"ì‚¬ì´íŠ¸ ìˆ˜": len(data), "ë°©ë²•": method_name}
                        )
                        self._was_failing = False
                    
                    return True, data, f"{method_name} ì„±ê³µ"
                else:
                    logger.warning(f"âŒ {method_name} ì‹¤íŒ¨: {message}")
                    
            except Exception as e:
                logger.error(f"âŒ {method_name} ì˜ˆì™¸ ë°œìƒ: {str(e)}")
                continue
        
        # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨
        self._was_failing = True
        self._handle_complete_failure()
        return False, [], "ëª¨ë“  í¬ë¡¤ë§ ë°©ë²• ì‹¤íŒ¨"
    
    def _crawl_with_cloudscraper(self) -> Tuple[bool, List[Dict], str]:
        """CloudScraperë¥¼ ì‚¬ìš©í•œ í¬ë¡¤ë§ (ê¸°ë³¸ ë°©ë²•)"""
        for attempt in range(self.retry_count):
            try:
                # ìš”ì²­ ê°„ ëœë¤ ì§€ì—°
                if attempt > 0:
                    delay = random.uniform(2, 5)
                    logger.info(f"ì¬ì‹œë„ ì „ {delay:.1f}ì´ˆ ëŒ€ê¸°...")
                    time.sleep(delay)
                
                response = self.scraper.get(
                    'https://www.pokerscout.com',
                    timeout=self.timeout
                )
                
                if response.status_code == 200:
                    data = self._parse_html(response.text)
                    if data:
                        return True, data, "ì„±ê³µ"
                    else:
                        logger.warning("HTML íŒŒì‹± ì‹¤íŒ¨ - í˜ì´ì§€ êµ¬ì¡° ë³€ê²½ ê°€ëŠ¥ì„±")
                elif response.status_code == 403:
                    return False, [], "403 Forbidden - ì°¨ë‹¨ë¨"
                else:
                    logger.warning(f"HTTP {response.status_code}")
                    
            except Exception as e:
                logger.error(f"CloudScraper ì‹œë„ {attempt+1} ì‹¤íŒ¨: {e}")
        
        return False, [], "CloudScraper ëª¨ë“  ì‹œë„ ì‹¤íŒ¨"
    
    def _crawl_with_requests(self) -> Tuple[bool, List[Dict], str]:
        """ì¼ë°˜ requests ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        try:
            session = requests.Session()
            response = session.get(
                'https://www.pokerscout.com',
                headers=headers,
                timeout=self.timeout
            )
            
            if response.status_code == 200:
                data = self._parse_html(response.text)
                if data:
                    return True, data, "ì„±ê³µ"
            
            return False, [], f"HTTP {response.status_code}"
            
        except Exception as e:
            return False, [], str(e)
    
    def _crawl_with_custom_headers(self) -> Tuple[bool, List[Dict], str]:
        """ë‹¤ì–‘í•œ User-Agentì™€ í—¤ë” ì¡°í•© ì‹œë„"""
        user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0'
        ]
        
        for ua in user_agents:
            try:
                headers = {
                    'User-Agent': ua,
                    'Accept': '*/*',
                    'Cache-Control': 'no-cache',
                    'Pragma': 'no-cache'
                }
                
                response = requests.get(
                    'https://www.pokerscout.com',
                    headers=headers,
                    timeout=self.timeout
                )
                
                if response.status_code == 200:
                    data = self._parse_html(response.text)
                    if data:
                        return True, data, f"ì„±ê³µ (UA: {ua[:30]}...)"
                        
            except Exception as e:
                continue
        
        return False, [], "ëª¨ë“  User-Agent ì‹¤íŒ¨"
    
    def _use_cached_data(self) -> Tuple[bool, List[Dict], str]:
        """ë§ˆì§€ë§‰ ì„±ê³µí•œ ë°ì´í„° ì‚¬ìš© (ë¹„ìƒìš©)"""
        if self.last_successful_data and self.last_successful_time:
            time_diff = datetime.now() - self.last_successful_time
            hours_old = time_diff.total_seconds() / 3600
            
            if hours_old < 24:  # 24ì‹œê°„ ì´ë‚´ ë°ì´í„°ë§Œ ì‚¬ìš©
                logger.warning(f"âš ï¸ {hours_old:.1f}ì‹œê°„ ì „ ìºì‹œ ë°ì´í„° ì‚¬ìš©")
                
                # ìºì‹œ ë°ì´í„° ì‚¬ìš© ì•Œë¦¼
                self.alert_system.send_alert(
                    "ìºì‹œ ë°ì´í„° ì‚¬ìš© ì¤‘",
                    f"ì‹¤ì‹œê°„ í¬ë¡¤ë§ ì‹¤íŒ¨ë¡œ {hours_old:.1f}ì‹œê°„ ì „ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.",
                    "WARNING",
                    {
                        "ìºì‹œ ì‹œê°„": self.last_successful_time.strftime("%Y-%m-%d %H:%M:%S"),
                        "ë°ì´í„° ìˆ˜": len(self.last_successful_data)
                    }
                )
                
                return True, self.last_successful_data, f"ìºì‹œ ë°ì´í„° ({hours_old:.1f}ì‹œê°„ ì „)"
        
        return False, [], "ì‚¬ìš© ê°€ëŠ¥í•œ ìºì‹œ ì—†ìŒ"
    
    def _parse_html(self, html_content: str) -> List[Dict]:
        """HTML íŒŒì‹± ë¡œì§"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ í…Œì´ë¸” ì°¾ê¸°
            table = soup.find('table', {'class': 'rankTable'})
            
            if not table:
                # ëŒ€ì²´ ì„ íƒì ì‹œë„
                table = soup.find('table', class_=re.compile('rank', re.I))
            
            if not table:
                tables = soup.find_all('table')
                if tables:
                    table = tables[0]  # ì²« ë²ˆì§¸ í…Œì´ë¸” ì‚¬ìš©
                    logger.warning(f"rankTableì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì²« ë²ˆì§¸ í…Œì´ë¸” ì‚¬ìš© (ì´ {len(tables)}ê°œ)")
            
            if not table:
                logger.error("í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return []
            
            collected_data = []
            rows = table.find_all('tr')[1:]  # í—¤ë” ì œì™¸
            
            for row in rows:
                try:
                    # ê´‘ê³  í–‰ì´ë‚˜ íŠ¹ë³„ í–‰ ìŠ¤í‚µ
                    if self._is_advertisement_row(row):
                        logger.debug("ê´‘ê³  í–‰ ìŠ¤í‚µë¨")
                        continue
                    
                    # ìˆœìœ„ ë²ˆí˜¸ ì¶”ì¶œ
                    rank = self._extract_rank(row)
                    
                    # ì‚¬ì´íŠ¸ ì´ë¦„ ì¶”ì¶œ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
                    site_name = self._extract_site_name(row)
                    
                    if not site_name or len(site_name) < 2:
                        logger.debug(f"ì‚¬ì´íŠ¸ëª… ì¶”ì¶œ ì‹¤íŒ¨, í–‰ ìŠ¤í‚µ: {site_name}")
                        continue
                    
                    # WPT Global, GGPoker ON ë“± íŠ¹ì • ì‚¬ì´íŠ¸ í™•ì¸
                    if 'WPT' in site_name or 'GGPoker ON' in site_name:
                        logger.info(f"ì£¼ìš” ì‚¬ì´íŠ¸ ê°ì§€: {site_name}")
                    
                    # í†µê³„ ì¶”ì¶œ
                    stats = self._extract_stats(row)
                    
                    if stats['players_online'] == 0 and stats['cash_players'] == 0 and stats['peak_24h'] == 0:
                        continue
                    
                    # ì‚¬ì´íŠ¸ëª… ì •ë¦¬
                    site_name = self._clean_site_name(site_name)
                    
                    # ì¹´í…Œê³ ë¦¬ ê²°ì • (ë” í¬ê´„ì ì¸ ë§¤ì¹­)
                    category = self._determine_category(site_name)
                    
                    collected_data.append({
                        'rank': rank,
                        'site_name': site_name,
                        'category': category,
                        'players_online': stats['players_online'],
                        'cash_players': stats['cash_players'],
                        'peak_24h': stats['peak_24h'],
                        'seven_day_avg': stats['seven_day_avg'],
                        'collected_at': datetime.now(timezone.utc).isoformat()
                    })
                    
                    # ì£¼ìš” ì‚¬ì´íŠ¸ ë¡œê·¸
                    if category == 'GG_POKER' or 'WPT' in site_name:
                        logger.info(f"#{rank} {site_name}: {stats['players_online']:,}ëª… ì˜¨ë¼ì¸ ({category})")
                    
                except Exception as e:
                    logger.debug(f"í–‰ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            return collected_data
            
        except Exception as e:
            logger.error(f"HTML íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []
    
    def _extract_stats(self, row) -> Dict[str, int]:
        """í…Œì´ë¸” í–‰ì—ì„œ í†µê³„ ì¶”ì¶œ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)"""
        stats = {
            'players_online': 0,
            'cash_players': 0,
            'peak_24h': 0,
            'seven_day_avg': 0
        }
        
        try:
            # ë°©ë²• 1: IDë¡œ ì°¾ê¸°
            for stat_name, td_id in [
                ('players_online', 'online'),
                ('cash_players', 'cash'),
                ('peak_24h', 'peak'),
                ('seven_day_avg', 'avg')
            ]:
                td = row.find('td', {'id': td_id})
                if td:
                    # span íƒœê·¸ í™•ì¸
                    span = td.find('span')
                    text = span.get_text(strip=True) if span else td.get_text(strip=True)
                    text = text.replace(',', '')
                    if text.isdigit():
                        stats[stat_name] = int(text)
            
            # ë°©ë²• 2: ìˆœì„œë¡œ ì¶”ì¶œ (IDë¡œ ì°¾ê¸° ì‹¤íŒ¨ ì‹œ)
            if all(v == 0 for v in stats.values()):
                tds = row.find_all('td')
                if len(tds) >= 6:  # ìˆœìœ„, ì‚¬ì´íŠ¸ëª…, ì˜¨ë¼ì¸, ìºì‹œ, í”¼í¬, í‰ê· 
                    stat_indices = [2, 3, 4, 5]  # ì˜¨ë¼ì¸, ìºì‹œ, í”¼í¬, í‰ê·  ìˆœì„œ
                    stat_names = ['players_online', 'cash_players', 'peak_24h', 'seven_day_avg']
                    
                    for i, stat_name in enumerate(stat_names):
                        if stat_indices[i] < len(tds):
                            td = tds[stat_indices[i]]
                            text = td.get_text(strip=True).replace(',', '')
                            # ìˆ«ìë§Œ ì¶”ì¶œ
                            numbers = re.findall(r'\d+', text)
                            if numbers:
                                stats[stat_name] = int(numbers[0])
            
            # ë°©ë²• 3: í´ë˜ìŠ¤ëª…ìœ¼ë¡œ ì°¾ê¸°
            if all(v == 0 for v in stats.values()):
                for stat_name in ['players_online', 'cash_players', 'peak_24h', 'seven_day_avg']:
                    # ì¼ë°˜ì ì¸ í´ë˜ìŠ¤ëª… íŒ¨í„´ë“¤
                    class_patterns = [
                        f'{stat_name}', stat_name.replace('_', '-'), 
                        stat_name.split('_')[0], 'stat', 'number'
                    ]
                    
                    for pattern in class_patterns:
                        td = row.find('td', class_=re.compile(pattern, re.I))
                        if td:
                            text = td.get_text(strip=True).replace(',', '')
                            numbers = re.findall(r'\d+', text)
                            if numbers:
                                stats[stat_name] = int(numbers[0])
                                break
            
            # ë¡œê¹…
            if any(v > 0 for v in stats.values()):
                logger.debug(f"í†µê³„ ì¶”ì¶œ ì„±ê³µ: {stats}")
            else:
                logger.debug("í†µê³„ ì¶”ì¶œ ì‹¤íŒ¨ - ëª¨ë“  ê°’ì´ 0")
                
        except Exception as e:
            logger.debug(f"í†µê³„ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return stats
    
    def _is_advertisement_row(self, row) -> bool:
        """ê´‘ê³  í–‰ì´ë‚˜ íŠ¹ë³„í•œ í–‰ì¸ì§€ í™•ì¸"""
        try:
            # ê´‘ê³  ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
            text_content = row.get_text(strip=True).lower()
            
            # ì¼ë°˜ì ì¸ ê´‘ê³  í‚¤ì›Œë“œë“¤
            ad_keywords = [
                'best bonus', 'bonus', 'advertisement', 'ad', 'promo',
                'promotion', 'sponsor', 'featured', 'coinpoker'
            ]
            
            for keyword in ad_keywords:
                if keyword in text_content:
                    return True
            
            # ë§í¬ë‚˜ ê´‘ê³  classê°€ ìˆëŠ”ì§€ í™•ì¸
            if row.find('a', class_=re.compile('ad|bonus|promo', re.I)):
                return True
            
            # ë¹„ì •ìƒì ìœ¼ë¡œ ë§ì€ ë§í¬ê°€ ìˆëŠ” í–‰ (ê´‘ê³  ê°€ëŠ¥ì„±)
            links = row.find_all('a')
            if len(links) > 3:
                return True
            
            # í†µê³„ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ë¹„ì •ìƒì ì¸ í–‰
            stats_cells = row.find_all('td')
            if len(stats_cells) < 4:  # ìµœì†Œ 4ê°œ ì»¬ëŸ¼(ìˆœìœ„, ì‚¬ì´íŠ¸ëª…, í”Œë ˆì´ì–´ìˆ˜, í†µê³„) í•„ìš”
                return True
                
            return False
            
        except Exception as e:
            logger.debug(f"ê´‘ê³  í–‰ ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _extract_rank(self, row) -> int:
        """ìˆœìœ„ ë²ˆí˜¸ ì¶”ì¶œ"""
        try:
            # ì²« ë²ˆì§¸ tdì—ì„œ ìˆœìœ„ ì°¾ê¸°
            first_td = row.find('td')
            if first_td:
                # ìˆ«ìë§Œ ì¶”ì¶œ
                rank_text = first_td.get_text(strip=True)
                # ìˆœìœ„ ë²ˆí˜¸ íŒ¨í„´ ë§¤ì¹­ (1, 2, 3... ë˜ëŠ” #1, #2, #3...)
                rank_match = re.search(r'#?(\d+)', rank_text)
                if rank_match:
                    return int(rank_match.group(1))
            
            # ëŒ€ì•ˆ: classë‚˜ idë¡œ ìˆœìœ„ ì…€ ì°¾ê¸°
            rank_cell = row.find('td', class_=re.compile('rank|position', re.I))
            if rank_cell:
                rank_text = rank_cell.get_text(strip=True)
                rank_match = re.search(r'(\d+)', rank_text)
                if rank_match:
                    return int(rank_match.group(1))
            
            return 0  # ìˆœìœ„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ
            
        except Exception as e:
            logger.debug(f"ìˆœìœ„ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return 0
    
    def _extract_site_name(self, row) -> str:
        """ì‚¬ì´íŠ¸ ì´ë¦„ ì¶”ì¶œ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)"""
        try:
            # ë°©ë²• 1: brand-title class
            brand_title = row.find('span', {'class': 'brand-title'})
            if brand_title:
                site_name = brand_title.get_text(strip=True)
                if site_name and len(site_name) > 1:
                    return site_name
            
            # ë°©ë²• 2: brand ê´€ë ¨ class ê²€ìƒ‰
            brand_span = row.find('span', class_=re.compile('brand', re.I))
            if brand_span:
                site_name = brand_span.get_text(strip=True)
                if site_name and len(site_name) > 1:
                    return site_name
            
            # ë°©ë²• 3: ì‚¬ì´íŠ¸ ë§í¬ì—ì„œ ì¶”ì¶œ
            site_link = row.find('a', class_=re.compile('site|brand', re.I))
            if site_link:
                site_name = site_link.get_text(strip=True)
                if site_name and len(site_name) > 1:
                    return site_name
            
            # ë°©ë²• 4: ë‘ ë²ˆì§¸ tdì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìˆœìœ„ ë‹¤ìŒ ì…€)
            tds = row.find_all('td')
            if len(tds) >= 2:
                site_cell = tds[1]  # ë‘ ë²ˆì§¸ ì…€ (ì²« ë²ˆì§¸ëŠ” ë³´í†µ ìˆœìœ„)
                # ë§í¬ê°€ ìˆìœ¼ë©´ ë§í¬ í…ìŠ¤íŠ¸ ì‚¬ìš©
                link = site_cell.find('a')
                if link:
                    site_name = link.get_text(strip=True)
                else:
                    site_name = site_cell.get_text(strip=True)
                
                if site_name and len(site_name) > 1:
                    return site_name
            
            # ë°©ë²• 5: ê°•ë ¥í•œ íŒ¨í„´ìœ¼ë¡œ ì‚¬ì´íŠ¸ëª… ê²€ìƒ‰
            all_text = row.get_text()
            # ì•Œë ¤ì§„ í¬ì»¤ ì‚¬ì´íŠ¸ íŒ¨í„´ë“¤
            poker_sites = [
                r'PokerStars?', r'GGPoker(?:\s+ON)?', r'WPT\s+Global?', 
                r'partypoker', r'888poker', r'BetMGM', r'Natural8',
                r'CoinPoker', r'Ignition', r'BetOnline', r'Americas Cardroom',
                r'Bodog', r'SportsBetting', r'Bovada', r'WSOP'
            ]
            
            for pattern in poker_sites:
                match = re.search(pattern, all_text, re.IGNORECASE)
                if match:
                    return match.group(0).strip()
            
            return ""
            
        except Exception as e:
            logger.debug(f"ì‚¬ì´íŠ¸ëª… ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return ""
    
    def _clean_site_name(self, site_name: str) -> str:
        """ì‚¬ì´íŠ¸ëª… ì •ë¦¬ ë° í‘œì¤€í™”"""
        if not site_name:
            return ""
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í—ˆìš©: ë¬¸ì, ìˆ«ì, ê³µë°±, í•˜ì´í”ˆ, ê´„í˜¸, ì , &)
        cleaned = re.sub(r'[^\w\s\-\(\)\.&]', '', site_name).strip()
        
        # ì—°ì† ê³µë°± ì œê±°
        cleaned = re.sub(r'\s+', ' ', cleaned)
        
        # ì•Œë ¤ì§„ ì‚¬ì´íŠ¸ëª… í‘œì¤€í™”
        standardizations = {
            'gg poker': 'GGPoker',
            'ggpoker on': 'GGPoker ON',
            'wpt global': 'WPT Global',
            'pokerstars': 'PokerStars',
            'party poker': 'partypoker',
            '888 poker': '888poker'
        }
        
        cleaned_lower = cleaned.lower()
        for old, new in standardizations.items():
            if old in cleaned_lower:
                cleaned = new
                break
        
        return cleaned
    
    def _determine_category(self, site_name: str) -> str:
        """ì‚¬ì´íŠ¸ ì¹´í…Œê³ ë¦¬ ê²°ì • (ë” í¬ê´„ì ì¸ ë§¤ì¹­)"""
        if not site_name:
            return 'UNKNOWN'
        
        site_lower = site_name.lower()
        
        # GG Network ê´€ë ¨ ì‚¬ì´íŠ¸ë“¤ (ë” í¬ê´„ì )
        gg_patterns = [
            'gg', 'natural8', 'betkings', 'ggasia', 'ggpoker'
        ]
        
        for pattern in gg_patterns:
            if pattern in site_lower:
                return 'GG_POKER'
        
        # ë‹¤ë¥¸ ì£¼ìš” ë„¤íŠ¸ì›Œí¬ë“¤
        if any(keyword in site_lower for keyword in ['pokerstars', 'stars']):
            return 'POKERSTARS'
        elif any(keyword in site_lower for keyword in ['wpt', 'world poker tour']):
            return 'WPT'
        elif any(keyword in site_lower for keyword in ['party', 'bwin']):
            return 'PARTY_POKER'
        
        return 'COMPETITOR'
    
    def _handle_complete_failure(self):
        """ëª¨ë“  í¬ë¡¤ë§ ë°©ë²• ì‹¤íŒ¨ ì‹œ ì²˜ë¦¬"""
        error_details = {
            "ì‹œê°„": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "URL": "https://www.pokerscout.com",
            "ì‹œë„í•œ ë°©ë²•": "CloudScraper, Requests, Custom Headers, Cache",
            "ì•¡ì…˜ í•„ìš”": "í¬ë¡¤ë§ ë¡œì§ ì ê²€ í•„ìš”"
        }
        
        # Critical ì•Œë¦¼ ë°œì†¡
        self.alert_system.send_alert(
            "PokerScout í¬ë¡¤ë§ ì™„ì „ ì‹¤íŒ¨",
            "ëª¨ë“  í¬ë¡¤ë§ ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤.",
            "CRITICAL",
            error_details
        )
        
        # ë°±ì—… JSON íŒŒì¼ ìƒì„±
        self._save_failure_report(error_details)
    
    def _save_failure_report(self, details: Dict):
        """ì‹¤íŒ¨ ë¦¬í¬íŠ¸ ì €ì¥"""
        report = {
            "status": "FAILED",
            "timestamp": datetime.now().isoformat(),
            "details": details,
            "last_successful": {
                "time": self.last_successful_time.isoformat() if self.last_successful_time else None,
                "data_count": len(self.last_successful_data) if self.last_successful_data else 0
            }
        }
        
        filename = f"crawl_failure_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ì‹¤íŒ¨ ë¦¬í¬íŠ¸ ì €ì¥: {filename}")

def get_access_token():
    """Firebase ì•¡ì„¸ìŠ¤ í† í° ìƒì„±"""
    try:
        possible_key_paths = [
            os.path.join(os.path.dirname(os.path.abspath(__file__)), 'key', 'firebase-service-account-key.json'),
            os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'key', 'firebase-service-account-key.json'),
        ]
        
        key_path = None
        for path in possible_key_paths:
            if os.path.exists(path):
                key_path = path
                logger.info(f"Firebase í‚¤ íŒŒì¼ ë°œê²¬: {key_path}")
                break
        
        if not key_path:
            logger.warning("Firebase í‚¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        credentials = service_account.Credentials.from_service_account_file(
            key_path,
            scopes=['https://www.googleapis.com/auth/datastore']
        )
        
        credentials.refresh(Request())
        return credentials.token
        
    except Exception as e:
        logger.error(f"ì•¡ì„¸ìŠ¤ í† í° ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def upload_to_realtime_database(data):
    """Realtime Databaseì— ë°ì´í„° ì—…ë¡œë“œ (GitHub Pagesìš©)"""
    if not data:
        return False
    
    try:
        # í˜„ì¬ ë‚ ì§œë¥¼ í‚¤ë¡œ ì‚¬ìš©
        date_key = datetime.now().strftime("%Y-%m-%d")
        
        # ë°ì´í„° êµ¬ì¡° ìƒì„±
        db_data = {
            "date": date_key,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "sites": []
        }
        
        # ì‚¬ì´íŠ¸ ë°ì´í„° ë³€í™˜
        for site in data:
            site_data = {
                "siteName": site.get('site_name', ''),
                "category": site.get('category', 'COMPETITOR'),
                "rank": site.get('rank', 999),
                "cashPlayers": site.get('cash_players', 0),
                "playersOnline": site.get('players_online', 0),
                "peak24h": site.get('peak_24h', 0),
                "sevenDayAvg": site.get('seven_day_avg', 0)
            }
            db_data["sites"].append(site_data)
        
        # ì „ì²´ í†µê³„ ì¶”ê°€
        total_cash = sum(s.get('cash_players', 0) for s in data)
        total_online = sum(s.get('players_online', 0) for s in data)
        gg_sites = [s for s in data if s.get('category') == 'GG_POKER']
        gg_total = sum(s.get('cash_players', 0) for s in gg_sites)
        
        db_data["summary"] = {
            "totalSites": len(data),
            "totalCashPlayers": total_cash,
            "totalOnlinePlayers": total_online,
            "ggNetworkSites": len(gg_sites),
            "ggNetworkPlayers": gg_total,
            "ggNetworkShare": round((gg_total / total_cash * 100) if total_cash > 0 else 0, 1)
        }
        
        # Realtime Databaseì— ì—…ë¡œë“œ
        # 1. pokerData/{date} ê²½ë¡œì— ì €ì¥
        url = f"{FIREBASE_REALTIME_DB_URL}/pokerData/{date_key}.json"
        response = requests.put(url, json=db_data)
        
        if response.status_code == 200:
            logger.info(f"âœ… Realtime Database ì—…ë¡œë“œ ì„±ê³µ: {date_key}")
            
            # 2. latest ê²½ë¡œì—ë„ ì €ì¥ (ìµœì‹  ë°ì´í„° ë¹ ë¥¸ ì ‘ê·¼ìš©)
            latest_url = f"{FIREBASE_REALTIME_DB_URL}/latest.json"
            requests.put(latest_url, json=db_data)
            
            return True
        else:
            logger.error(f"âŒ Realtime Database ì—…ë¡œë“œ ì‹¤íŒ¨: {response.status_code}")
            return False
            
    except Exception as e:
        logger.error(f"Realtime Database ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def upload_to_firestore_rest(data, access_token=None):
    """Firestoreì— ë°ì´í„° ì—…ë¡œë“œ"""
    if not data:
        logger.warning("ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    logger.info(f"Firestoreì— {len(data)}ê°œ ì‚¬ì´íŠ¸ ë°ì´í„° ì—…ë¡œë“œ ì‹œì‘...")
    
    headers = {'Content-Type': 'application/json'}
    if access_token:
        headers['Authorization'] = f'Bearer {access_token}'
    
    success_count = 0
    
    try:
        for site_data in data:
            site_name = site_data['site_name']
            collected_at_iso = site_data['collected_at']
            
            # sites ì»¬ë ‰ì…˜ ì—…ë°ì´íŠ¸ (ìˆœìœ„ ì •ë³´ í¬í•¨)
            site_url = f"{FIRESTORE_BASE_URL}/sites/{site_name}"
            site_doc = {
                "fields": {
                    "site_name": {"stringValue": site_name},
                    "category": {"stringValue": site_data['category']},
                    "rank": {"integerValue": str(site_data.get('rank', 999))},  # ìˆœìœ„ ì¶”ê°€
                    "last_updated_at": {"timestampValue": datetime.now(timezone.utc).isoformat()}
                }
            }
            
            response = requests.patch(site_url, json=site_doc, headers=headers)
            
            if response.status_code not in [200, 204]:
                logger.warning(f"ì‚¬ì´íŠ¸ ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ ({site_name}): {response.status_code}")
                continue
            
            # traffic_logs í•˜ìœ„ ì»¬ë ‰ì…˜ì— ì¶”ê°€
            doc_id = collected_at_iso
            traffic_url = f"{FIRESTORE_BASE_URL}/sites/{site_name}/traffic_logs/{doc_id}"
            traffic_doc = {
                "fields": {
                    "rank": {"integerValue": str(site_data.get('rank', 0))},
                    "players_online": {"integerValue": str(site_data['players_online'])},
                    "cash_players": {"integerValue": str(site_data['cash_players'])},
                    "peak_24h": {"integerValue": str(site_data['peak_24h'])},
                    "seven_day_avg": {"integerValue": str(site_data['seven_day_avg'])},
                    "collected_at": {"timestampValue": collected_at_iso}
                }
            }
            
            response = requests.patch(traffic_url, json=traffic_doc, headers=headers)
            
            if response.status_code in [200, 201]:
                success_count += 1
            else:
                logger.warning(f"íŠ¸ë˜í”½ ë¡œê·¸ ìƒì„± ì‹¤íŒ¨ ({site_name}): {response.status_code}")
        
        logger.info(f"âœ… Firestore ì—…ë¡œë“œ ì™„ë£Œ: {success_count}/{len(data)}ê°œ ì„±ê³µ")
        return success_count > 0
        
    except Exception as e:
        logger.error(f"Firestore ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def run_enhanced_crawl():
    """í–¥ìƒëœ í¬ë¡¤ë§ ì‹¤í–‰ ë©”ì¸ í•¨ìˆ˜"""
    logger.info("="*80)
    logger.info("í–¥ìƒëœ PokerScout í¬ë¡¤ë§ ì‹œì‘")
    logger.info(f"ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("="*80)
    
    try:
        # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        crawler = RobustPokerScoutCrawler()
        
        # í´ë°± ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ í¬ë¡¤ë§ ì‹œë„
        success, crawled_data, method = crawler.crawl_with_fallback()
        
        if success and crawled_data:
            logger.info(f"âœ… í¬ë¡¤ë§ ì„±ê³µ: {len(crawled_data)}ê°œ ì‚¬ì´íŠ¸ ({method})")
            
            # ì£¼ìš” í†µê³„ ì¶œë ¥
            total_players = sum(site.get('players_online', 0) for site in crawled_data)
            logger.info(f"ğŸ“Š ì „ì²´ ì˜¨ë¼ì¸ í”Œë ˆì´ì–´: {total_players:,}ëª…")
            
            # ìƒìœ„ 3ê°œ ì‚¬ì´íŠ¸ ì¶œë ¥
            for i, site in enumerate(crawled_data[:3], 1):
                logger.info(
                    f"  {i}. {site['site_name']}: "
                    f"{site['players_online']:,}ëª… ì˜¨ë¼ì¸"
                )
            
            # Firebase ì—…ë¡œë“œ (ë‘ ê°€ì§€ ë°©ì‹ ëª¨ë‘ ì‹œë„)
            # 1. Realtime Database ì—…ë¡œë“œ (GitHub Pagesìš©)
            realtime_success = upload_to_realtime_database(crawled_data)
            
            # 2. Firestore ì—…ë¡œë“œ (ê¸°ì¡´ ë°©ì‹)
            access_token = get_access_token()
            upload_success = upload_to_firestore_rest(crawled_data, access_token)
            
            if not upload_success:
                # ì—…ë¡œë“œ ì‹¤íŒ¨ ì•Œë¦¼
                crawler.alert_system.send_alert(
                    "Firebase ì—…ë¡œë“œ ì‹¤íŒ¨",
                    "ë°ì´í„° ìˆ˜ì§‘ì€ ì„±ê³µí–ˆìœ¼ë‚˜ Firebase ì—…ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                    "ERROR",
                    {"ìˆ˜ì§‘ ë°ì´í„°": len(crawled_data), "ë°©ë²•": method}
                )
                
                # ë°±ì—… ì €ì¥
                backup_file = f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                with open(backup_file, 'w', encoding='utf-8') as f:
                    json.dump(crawled_data, f, ensure_ascii=False, indent=2)
                logger.info(f"ë°±ì—… íŒŒì¼ ì €ì¥: {backup_file}")
            
            return True
            
        else:
            logger.error("âŒ í¬ë¡¤ë§ ì™„ì „ ì‹¤íŒ¨")
            return False
            
    except Exception as e:
        logger.error(f"ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        
        # ì˜ˆì™¸ ë°œìƒ ì•Œë¦¼
        AlertSystem().send_alert(
            "í¬ë¡¤ëŸ¬ ì˜ˆì™¸ ë°œìƒ",
            f"ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "CRITICAL",
            {"ì—ëŸ¬ íƒ€ì…": type(e).__name__}
        )
        
        return False

if __name__ == "__main__":
    success = run_enhanced_crawl()
    logger.info("="*80)
    logger.info(f"í¬ë¡¤ë§ ì¢…ë£Œ - ê²°ê³¼: {'ì„±ê³µ' if success else 'ì‹¤íŒ¨'}")
    logger.info("="*80)
    sys.exit(0 if success else 1)