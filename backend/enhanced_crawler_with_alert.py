#!/usr/bin/env python3
"""
í–¥ìƒëœ PokerScout í¬ë¡¤ëŸ¬ - ë‹¤ì¤‘ í´ë°± ë©”ì»¤ë‹ˆì¦˜ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ í¬í•¨
- CloudScraper â†’ Selenium â†’ ì¼ë°˜ requests ìˆœì„œë¡œ ì‹œë„
- ì‹¤íŒ¨ ì‹œ ì´ë©”ì¼/Discord/Slack ì•Œë¦¼ ë°œì†¡
- ìƒì„¸í•œ ì—ëŸ¬ ë¡œê¹… ë° ë³µêµ¬ ì‹œë„
"""
import sys
import os
import logging
from datetime import datetime, timezone, timedelta
import json
import re
import time
import random
from typing import List, Dict, Optional, Tuple
import requests
from bs4 import BeautifulSoup
from google.oauth2 import service_account
from google.auth.transport.requests import Request
import cloudscraper

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(funcName)s] %(message)s'
)
logger = logging.getLogger(__name__)

# Firebase í”„ë¡œì íŠ¸ ì„¤ì •
FIREBASE_PROJECT_ID = "poker-online-analyze"
FIRESTORE_BASE_URL = f"https://firestore.googleapis.com/v1/projects/{FIREBASE_PROJECT_ID}/databases/(default)/documents"

# ì•Œë¦¼ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì½ê¸°)
ALERT_CONFIG = {
    'discord_webhook': os.environ.get('DISCORD_WEBHOOK_URL'),
    'slack_webhook': os.environ.get('SLACK_WEBHOOK_URL'),
    'email_to': os.environ.get('ALERT_EMAIL'),
    'github_issue': os.environ.get('GITHUB_REPO')  # e.g., "garimto81/poker-online-analyze"
}

class AlertSystem:
    """ë‹¤ì–‘í•œ ì±„ë„ë¡œ ì•Œë¦¼ì„ ë³´ë‚´ëŠ” ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.alerts_sent = []
        self.last_alert_time = None
        
    def send_alert(self, title: str, message: str, level: str = "ERROR", details: Dict = None):
        """ëª¨ë“  ì„¤ì •ëœ ì±„ë„ë¡œ ì•Œë¦¼ ë°œì†¡"""
        # ì¤‘ë³µ ì•Œë¦¼ ë°©ì§€ (5ë¶„ ì´ë‚´ ë™ì¼ ì•Œë¦¼ ì°¨ë‹¨)
        alert_key = f"{title}:{level}"
        current_time = datetime.now()
        
        if self.last_alert_time:
            time_diff = (current_time - self.last_alert_time).seconds
            if time_diff < 300 and alert_key in self.alerts_sent:  # 5ë¶„
                logger.info(f"ì•Œë¦¼ ìŠ¤í‚µ (ìµœê·¼ ë°œì†¡ë¨): {title}")
                return
        
        self.last_alert_time = current_time
        self.alerts_sent.append(alert_key)
        
        # Discord ì•Œë¦¼
        if ALERT_CONFIG.get('discord_webhook'):
            self._send_discord_alert(title, message, level, details)
        
        # Slack ì•Œë¦¼
        if ALERT_CONFIG.get('slack_webhook'):
            self._send_slack_alert(title, message, level, details)
        
        # GitHub Issue ìƒì„±
        if ALERT_CONFIG.get('github_issue') and level == "CRITICAL":
            self._create_github_issue(title, message, details)
        
        # ì½˜ì†” ì¶œë ¥ (GitHub Actions ë¡œê·¸)
        self._log_to_console(title, message, level, details)
    
    def _send_discord_alert(self, title: str, message: str, level: str, details: Dict):
        """Discord ì›¹í›…ìœ¼ë¡œ ì•Œë¦¼ ë°œì†¡"""
        try:
            color = {
                "INFO": 0x00FF00,     # ë…¹ìƒ‰
                "WARNING": 0xFFFF00,  # ë…¸ë€ìƒ‰
                "ERROR": 0xFF0000,    # ë¹¨ê°„ìƒ‰
                "CRITICAL": 0x8B0000  # ì§„í•œ ë¹¨ê°„ìƒ‰
            }.get(level, 0x808080)
            
            embed = {
                "title": f"ğŸš¨ {title}",
                "description": message,
                "color": color,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "fields": []
            }
            
            if details:
                for key, value in details.items():
                    embed["fields"].append({
                        "name": key,
                        "value": str(value)[:1024],
                        "inline": True
                    })
            
            payload = {"embeds": [embed]}
            response = requests.post(
                ALERT_CONFIG['discord_webhook'],
                json=payload,
                timeout=10
            )
            
            if response.status_code == 204:
                logger.info("Discord ì•Œë¦¼ ë°œì†¡ ì„±ê³µ")
            else:
                logger.warning(f"Discord ì•Œë¦¼ ì‹¤íŒ¨: {response.status_code}")
                
        except Exception as e:
            logger.error(f"Discord ì•Œë¦¼ ë°œì†¡ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _send_slack_alert(self, title: str, message: str, level: str, details: Dict):
        """Slack ì›¹í›…ìœ¼ë¡œ ì•Œë¦¼ ë°œì†¡"""
        try:
            blocks = [
                {
                    "type": "header",
                    "text": {
                        "type": "plain_text",
                        "text": f"ğŸš¨ {title}"
                    }
                },
                {
                    "type": "section",
                    "text": {
                        "type": "mrkdwn",
                        "text": f"*Level:* {level}\n*Message:* {message}"
                    }
                }
            ]
            
            if details:
                fields = []
                for key, value in details.items():
                    fields.append({
                        "type": "mrkdwn",
                        "text": f"*{key}:*\n{value}"
                    })
                blocks.append({
                    "type": "section",
                    "fields": fields[:10]  # ìµœëŒ€ 10ê°œ í•„ë“œ
                })
            
            payload = {"blocks": blocks}
            response = requests.post(
                ALERT_CONFIG['slack_webhook'],
                json=payload,
                timeout=10
            )
            
            if response.status_code == 200:
                logger.info("Slack ì•Œë¦¼ ë°œì†¡ ì„±ê³µ")
            else:
                logger.warning(f"Slack ì•Œë¦¼ ì‹¤íŒ¨: {response.status_code}")
                
        except Exception as e:
            logger.error(f"Slack ì•Œë¦¼ ë°œì†¡ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _create_github_issue(self, title: str, message: str, details: Dict):
        """GitHub Issue ìë™ ìƒì„± (Critical ì—ëŸ¬ë§Œ)"""
        try:
            if not os.environ.get('GITHUB_TOKEN'):
                logger.warning("GitHub Tokenì´ ì„¤ì •ë˜ì§€ ì•Šì•„ Issueë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            repo = ALERT_CONFIG['github_issue']
            url = f"https://api.github.com/repos/{repo}/issues"
            
            body = f"## ìë™ ìƒì„±ëœ ì´ìŠˆ\n\n"
            body += f"**ë°œìƒ ì‹œê°„:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
            body += f"**ë©”ì‹œì§€:** {message}\n\n"
            
            if details:
                body += "### ìƒì„¸ ì •ë³´\n\n"
                for key, value in details.items():
                    body += f"- **{key}:** {value}\n"
            
            headers = {
                "Authorization": f"token {os.environ.get('GITHUB_TOKEN')}",
                "Accept": "application/vnd.github.v3+json"
            }
            
            payload = {
                "title": f"[ìë™] {title}",
                "body": body,
                "labels": ["bug", "automated", "crawler-issue"]
            }
            
            response = requests.post(url, json=payload, headers=headers, timeout=10)
            
            if response.status_code == 201:
                issue_url = response.json().get('html_url')
                logger.info(f"GitHub Issue ìƒì„±ë¨: {issue_url}")
            else:
                logger.warning(f"GitHub Issue ìƒì„± ì‹¤íŒ¨: {response.status_code}")
                
        except Exception as e:
            logger.error(f"GitHub Issue ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _log_to_console(self, title: str, message: str, level: str, details: Dict):
        """ì½˜ì†”ì— êµ¬ì¡°í™”ëœ ì•Œë¦¼ ì¶œë ¥ (GitHub Actions ë¡œê·¸ìš©)"""
        print("\n" + "="*80)
        print(f"ğŸš¨ ì•Œë¦¼: {title}")
        print(f"ğŸ“Š ë ˆë²¨: {level}")
        print(f"ğŸ“ ë©”ì‹œì§€: {message}")
        
        if details:
            print("ğŸ“‹ ìƒì„¸ ì •ë³´:")
            for key, value in details.items():
                print(f"  - {key}: {value}")
        
        print("="*80 + "\n")

class RobustPokerScoutCrawler:
    """ë‹¤ì¤‘ í´ë°± ë©”ì»¤ë‹ˆì¦˜ì„ ê°–ì¶˜ ê²¬ê³ í•œ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.alert_system = AlertSystem()
        self.scraper = cloudscraper.create_scraper(
            browser={'browser': 'chrome', 'platform': 'windows', 'mobile': False}
        )
        self.gg_poker_sites = ['GGNetwork', 'GGPoker ON', 'GG Poker', 'GGPoker']
        self.retry_count = 3
        self.timeout = 30
        self.last_successful_data = None
        self.last_successful_time = None
        
    def crawl_with_fallback(self) -> Tuple[bool, List[Dict], str]:
        """ë‹¤ì¤‘ í´ë°± ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ í¬ë¡¤ë§ ì‹œë„"""
        methods = [
            ("CloudScraper", self._crawl_with_cloudscraper),
            ("Regular Requests", self._crawl_with_requests),
            ("Different Headers", self._crawl_with_custom_headers),
            ("Cached Data", self._use_cached_data)
        ]
        
        for method_name, method_func in methods:
            logger.info(f"í¬ë¡¤ë§ ì‹œë„: {method_name}")
            
            try:
                success, data, message = method_func()
                
                if success and data:
                    logger.info(f"âœ… {method_name} ì„±ê³µ: {len(data)}ê°œ ì‚¬ì´íŠ¸")
                    
                    # ì„±ê³µ ì‹œ ìºì‹œ ì—…ë°ì´íŠ¸
                    self.last_successful_data = data
                    self.last_successful_time = datetime.now()
                    
                    # ì´ì „ì— ì‹¤íŒ¨í–ˆë‹¤ê°€ ë³µêµ¬ëœ ê²½ìš° ì•Œë¦¼
                    if hasattr(self, '_was_failing') and self._was_failing:
                        self.alert_system.send_alert(
                            "í¬ë¡¤ë§ ë³µêµ¬ë¨",
                            f"{method_name} ë°©ë²•ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘ì´ ë³µêµ¬ë˜ì—ˆìŠµë‹ˆë‹¤.",
                            "INFO",
                            {"ì‚¬ì´íŠ¸ ìˆ˜": len(data), "ë°©ë²•": method_name}
                        )
                        self._was_failing = False
                    
                    return True, data, f"{method_name} ì„±ê³µ"
                else:
                    logger.warning(f"âŒ {method_name} ì‹¤íŒ¨: {message}")
                    
            except Exception as e:
                logger.error(f"âŒ {method_name} ì˜ˆì™¸ ë°œìƒ: {str(e)}")
                continue
        
        # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨
        self._was_failing = True
        self._handle_complete_failure()
        return False, [], "ëª¨ë“  í¬ë¡¤ë§ ë°©ë²• ì‹¤íŒ¨"
    
    def _crawl_with_cloudscraper(self) -> Tuple[bool, List[Dict], str]:
        """CloudScraperë¥¼ ì‚¬ìš©í•œ í¬ë¡¤ë§ (ê¸°ë³¸ ë°©ë²•)"""
        for attempt in range(self.retry_count):
            try:
                # ìš”ì²­ ê°„ ëœë¤ ì§€ì—°
                if attempt > 0:
                    delay = random.uniform(2, 5)
                    logger.info(f"ì¬ì‹œë„ ì „ {delay:.1f}ì´ˆ ëŒ€ê¸°...")
                    time.sleep(delay)
                
                response = self.scraper.get(
                    'https://www.pokerscout.com',
                    timeout=self.timeout
                )
                
                if response.status_code == 200:
                    data = self._parse_html(response.text)
                    if data:
                        return True, data, "ì„±ê³µ"
                    else:
                        logger.warning("HTML íŒŒì‹± ì‹¤íŒ¨ - í˜ì´ì§€ êµ¬ì¡° ë³€ê²½ ê°€ëŠ¥ì„±")
                elif response.status_code == 403:
                    return False, [], "403 Forbidden - ì°¨ë‹¨ë¨"
                else:
                    logger.warning(f"HTTP {response.status_code}")
                    
            except Exception as e:
                logger.error(f"CloudScraper ì‹œë„ {attempt+1} ì‹¤íŒ¨: {e}")
        
        return False, [], "CloudScraper ëª¨ë“  ì‹œë„ ì‹¤íŒ¨"
    
    def _crawl_with_requests(self) -> Tuple[bool, List[Dict], str]:
        """ì¼ë°˜ requests ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        try:
            session = requests.Session()
            response = session.get(
                'https://www.pokerscout.com',
                headers=headers,
                timeout=self.timeout
            )
            
            if response.status_code == 200:
                data = self._parse_html(response.text)
                if data:
                    return True, data, "ì„±ê³µ"
            
            return False, [], f"HTTP {response.status_code}"
            
        except Exception as e:
            return False, [], str(e)
    
    def _crawl_with_custom_headers(self) -> Tuple[bool, List[Dict], str]:
        """ë‹¤ì–‘í•œ User-Agentì™€ í—¤ë” ì¡°í•© ì‹œë„"""
        user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0'
        ]
        
        for ua in user_agents:
            try:
                headers = {
                    'User-Agent': ua,
                    'Accept': '*/*',
                    'Cache-Control': 'no-cache',
                    'Pragma': 'no-cache'
                }
                
                response = requests.get(
                    'https://www.pokerscout.com',
                    headers=headers,
                    timeout=self.timeout
                )
                
                if response.status_code == 200:
                    data = self._parse_html(response.text)
                    if data:
                        return True, data, f"ì„±ê³µ (UA: {ua[:30]}...)"
                        
            except Exception as e:
                continue
        
        return False, [], "ëª¨ë“  User-Agent ì‹¤íŒ¨"
    
    def _use_cached_data(self) -> Tuple[bool, List[Dict], str]:
        """ë§ˆì§€ë§‰ ì„±ê³µí•œ ë°ì´í„° ì‚¬ìš© (ë¹„ìƒìš©)"""
        if self.last_successful_data and self.last_successful_time:
            time_diff = datetime.now() - self.last_successful_time
            hours_old = time_diff.total_seconds() / 3600
            
            if hours_old < 24:  # 24ì‹œê°„ ì´ë‚´ ë°ì´í„°ë§Œ ì‚¬ìš©
                logger.warning(f"âš ï¸ {hours_old:.1f}ì‹œê°„ ì „ ìºì‹œ ë°ì´í„° ì‚¬ìš©")
                
                # ìºì‹œ ë°ì´í„° ì‚¬ìš© ì•Œë¦¼
                self.alert_system.send_alert(
                    "ìºì‹œ ë°ì´í„° ì‚¬ìš© ì¤‘",
                    f"ì‹¤ì‹œê°„ í¬ë¡¤ë§ ì‹¤íŒ¨ë¡œ {hours_old:.1f}ì‹œê°„ ì „ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.",
                    "WARNING",
                    {
                        "ìºì‹œ ì‹œê°„": self.last_successful_time.strftime("%Y-%m-%d %H:%M:%S"),
                        "ë°ì´í„° ìˆ˜": len(self.last_successful_data)
                    }
                )
                
                return True, self.last_successful_data, f"ìºì‹œ ë°ì´í„° ({hours_old:.1f}ì‹œê°„ ì „)"
        
        return False, [], "ì‚¬ìš© ê°€ëŠ¥í•œ ìºì‹œ ì—†ìŒ"
    
    def _parse_html(self, html_content: str) -> List[Dict]:
        """HTML íŒŒì‹± ë¡œì§"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ í…Œì´ë¸” ì°¾ê¸°
            table = soup.find('table', {'class': 'rankTable'})
            
            if not table:
                # ëŒ€ì²´ ì„ íƒì ì‹œë„
                table = soup.find('table', class_=re.compile('rank', re.I))
            
            if not table:
                tables = soup.find_all('table')
                if tables:
                    table = tables[0]  # ì²« ë²ˆì§¸ í…Œì´ë¸” ì‚¬ìš©
                    logger.warning(f"rankTableì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì²« ë²ˆì§¸ í…Œì´ë¸” ì‚¬ìš© (ì´ {len(tables)}ê°œ)")
            
            if not table:
                logger.error("í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return []
            
            collected_data = []
            rows = table.find_all('tr')[1:]  # í—¤ë” ì œì™¸
            
            for row in rows:
                try:
                    # ì‚¬ì´íŠ¸ ì´ë¦„ ì¶”ì¶œ
                    brand_title = row.find('span', {'class': 'brand-title'})
                    if not brand_title:
                        brand_title = row.find('span', class_=re.compile('brand', re.I))
                    
                    if not brand_title:
                        continue
                    
                    site_name = brand_title.get_text(strip=True)
                    if not site_name or len(site_name) < 2:
                        continue
                    
                    # í†µê³„ ì¶”ì¶œ
                    stats = self._extract_stats(row)
                    
                    if stats['players_online'] == 0 and stats['cash_players'] == 0 and stats['peak_24h'] == 0:
                        continue
                    
                    site_name = re.sub(r'[^\w\s\-\(\)\.&]', '', site_name).strip()
                    category = 'GG_POKER' if site_name in self.gg_poker_sites else 'COMPETITOR'
                    
                    collected_data.append({
                        'site_name': site_name,
                        'category': category,
                        'players_online': stats['players_online'],
                        'cash_players': stats['cash_players'],
                        'peak_24h': stats['peak_24h'],
                        'seven_day_avg': stats['seven_day_avg'],
                        'collected_at': datetime.now(timezone.utc).isoformat()
                    })
                    
                except Exception as e:
                    logger.debug(f"í–‰ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            return collected_data
            
        except Exception as e:
            logger.error(f"HTML íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []
    
    def _extract_stats(self, row) -> Dict[str, int]:
        """í…Œì´ë¸” í–‰ì—ì„œ í†µê³„ ì¶”ì¶œ"""
        stats = {
            'players_online': 0,
            'cash_players': 0,
            'peak_24h': 0,
            'seven_day_avg': 0
        }
        
        # IDë¡œ ì°¾ê¸°
        for stat_name, td_id in [
            ('players_online', 'online'),
            ('cash_players', 'cash'),
            ('peak_24h', 'peak'),
            ('seven_day_avg', 'avg')
        ]:
            td = row.find('td', {'id': td_id})
            if td:
                # span íƒœê·¸ í™•ì¸
                span = td.find('span')
                text = span.get_text(strip=True) if span else td.get_text(strip=True)
                text = text.replace(',', '')
                if text.isdigit():
                    stats[stat_name] = int(text)
        
        return stats
    
    def _handle_complete_failure(self):
        """ëª¨ë“  í¬ë¡¤ë§ ë°©ë²• ì‹¤íŒ¨ ì‹œ ì²˜ë¦¬"""
        error_details = {
            "ì‹œê°„": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "URL": "https://www.pokerscout.com",
            "ì‹œë„í•œ ë°©ë²•": "CloudScraper, Requests, Custom Headers, Cache",
            "ì•¡ì…˜ í•„ìš”": "í¬ë¡¤ë§ ë¡œì§ ì ê²€ í•„ìš”"
        }
        
        # Critical ì•Œë¦¼ ë°œì†¡
        self.alert_system.send_alert(
            "PokerScout í¬ë¡¤ë§ ì™„ì „ ì‹¤íŒ¨",
            "ëª¨ë“  í¬ë¡¤ë§ ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤.",
            "CRITICAL",
            error_details
        )
        
        # ë°±ì—… JSON íŒŒì¼ ìƒì„±
        self._save_failure_report(error_details)
    
    def _save_failure_report(self, details: Dict):
        """ì‹¤íŒ¨ ë¦¬í¬íŠ¸ ì €ì¥"""
        report = {
            "status": "FAILED",
            "timestamp": datetime.now().isoformat(),
            "details": details,
            "last_successful": {
                "time": self.last_successful_time.isoformat() if self.last_successful_time else None,
                "data_count": len(self.last_successful_data) if self.last_successful_data else 0
            }
        }
        
        filename = f"crawl_failure_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ì‹¤íŒ¨ ë¦¬í¬íŠ¸ ì €ì¥: {filename}")

def get_access_token():
    """Firebase ì•¡ì„¸ìŠ¤ í† í° ìƒì„±"""
    try:
        possible_key_paths = [
            os.path.join(os.path.dirname(os.path.abspath(__file__)), 'key', 'firebase-service-account-key.json'),
            os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'key', 'firebase-service-account-key.json'),
        ]
        
        key_path = None
        for path in possible_key_paths:
            if os.path.exists(path):
                key_path = path
                logger.info(f"Firebase í‚¤ íŒŒì¼ ë°œê²¬: {key_path}")
                break
        
        if not key_path:
            logger.warning("Firebase í‚¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        credentials = service_account.Credentials.from_service_account_file(
            key_path,
            scopes=['https://www.googleapis.com/auth/datastore']
        )
        
        credentials.refresh(Request())
        return credentials.token
        
    except Exception as e:
        logger.error(f"ì•¡ì„¸ìŠ¤ í† í° ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def upload_to_firestore_rest(data, access_token=None):
    """Firestoreì— ë°ì´í„° ì—…ë¡œë“œ"""
    if not data:
        logger.warning("ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    logger.info(f"Firestoreì— {len(data)}ê°œ ì‚¬ì´íŠ¸ ë°ì´í„° ì—…ë¡œë“œ ì‹œì‘...")
    
    headers = {'Content-Type': 'application/json'}
    if access_token:
        headers['Authorization'] = f'Bearer {access_token}'
    
    success_count = 0
    
    try:
        for site_data in data:
            site_name = site_data['site_name']
            collected_at_iso = site_data['collected_at']
            
            # sites ì»¬ë ‰ì…˜ ì—…ë°ì´íŠ¸
            site_url = f"{FIRESTORE_BASE_URL}/sites/{site_name}"
            site_doc = {
                "fields": {
                    "site_name": {"stringValue": site_name},
                    "category": {"stringValue": site_data['category']},
                    "last_updated_at": {"timestampValue": datetime.now(timezone.utc).isoformat()}
                }
            }
            
            response = requests.patch(site_url, json=site_doc, headers=headers)
            
            if response.status_code not in [200, 204]:
                logger.warning(f"ì‚¬ì´íŠ¸ ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ ({site_name}): {response.status_code}")
                continue
            
            # traffic_logs í•˜ìœ„ ì»¬ë ‰ì…˜ì— ì¶”ê°€
            doc_id = collected_at_iso
            traffic_url = f"{FIRESTORE_BASE_URL}/sites/{site_name}/traffic_logs/{doc_id}"
            traffic_doc = {
                "fields": {
                    "players_online": {"integerValue": str(site_data['players_online'])},
                    "cash_players": {"integerValue": str(site_data['cash_players'])},
                    "peak_24h": {"integerValue": str(site_data['peak_24h'])},
                    "seven_day_avg": {"integerValue": str(site_data['seven_day_avg'])},
                    "collected_at": {"timestampValue": collected_at_iso}
                }
            }
            
            response = requests.patch(traffic_url, json=traffic_doc, headers=headers)
            
            if response.status_code in [200, 201]:
                success_count += 1
            else:
                logger.warning(f"íŠ¸ë˜í”½ ë¡œê·¸ ìƒì„± ì‹¤íŒ¨ ({site_name}): {response.status_code}")
        
        logger.info(f"âœ… Firestore ì—…ë¡œë“œ ì™„ë£Œ: {success_count}/{len(data)}ê°œ ì„±ê³µ")
        return success_count > 0
        
    except Exception as e:
        logger.error(f"Firestore ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def run_enhanced_crawl():
    """í–¥ìƒëœ í¬ë¡¤ë§ ì‹¤í–‰ ë©”ì¸ í•¨ìˆ˜"""
    logger.info("="*80)
    logger.info("í–¥ìƒëœ PokerScout í¬ë¡¤ë§ ì‹œì‘")
    logger.info(f"ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("="*80)
    
    try:
        # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        crawler = RobustPokerScoutCrawler()
        
        # í´ë°± ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ í¬ë¡¤ë§ ì‹œë„
        success, crawled_data, method = crawler.crawl_with_fallback()
        
        if success and crawled_data:
            logger.info(f"âœ… í¬ë¡¤ë§ ì„±ê³µ: {len(crawled_data)}ê°œ ì‚¬ì´íŠ¸ ({method})")
            
            # ì£¼ìš” í†µê³„ ì¶œë ¥
            total_players = sum(site.get('players_online', 0) for site in crawled_data)
            logger.info(f"ğŸ“Š ì „ì²´ ì˜¨ë¼ì¸ í”Œë ˆì´ì–´: {total_players:,}ëª…")
            
            # ìƒìœ„ 3ê°œ ì‚¬ì´íŠ¸ ì¶œë ¥
            for i, site in enumerate(crawled_data[:3], 1):
                logger.info(
                    f"  {i}. {site['site_name']}: "
                    f"{site['players_online']:,}ëª… ì˜¨ë¼ì¸"
                )
            
            # Firebase ì—…ë¡œë“œ
            access_token = get_access_token()
            upload_success = upload_to_firestore_rest(crawled_data, access_token)
            
            if not upload_success:
                # ì—…ë¡œë“œ ì‹¤íŒ¨ ì•Œë¦¼
                crawler.alert_system.send_alert(
                    "Firebase ì—…ë¡œë“œ ì‹¤íŒ¨",
                    "ë°ì´í„° ìˆ˜ì§‘ì€ ì„±ê³µí–ˆìœ¼ë‚˜ Firebase ì—…ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                    "ERROR",
                    {"ìˆ˜ì§‘ ë°ì´í„°": len(crawled_data), "ë°©ë²•": method}
                )
                
                # ë°±ì—… ì €ì¥
                backup_file = f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                with open(backup_file, 'w', encoding='utf-8') as f:
                    json.dump(crawled_data, f, ensure_ascii=False, indent=2)
                logger.info(f"ë°±ì—… íŒŒì¼ ì €ì¥: {backup_file}")
            
            return True
            
        else:
            logger.error("âŒ í¬ë¡¤ë§ ì™„ì „ ì‹¤íŒ¨")
            return False
            
    except Exception as e:
        logger.error(f"ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        
        # ì˜ˆì™¸ ë°œìƒ ì•Œë¦¼
        AlertSystem().send_alert(
            "í¬ë¡¤ëŸ¬ ì˜ˆì™¸ ë°œìƒ",
            f"ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "CRITICAL",
            {"ì—ëŸ¬ íƒ€ì…": type(e).__name__}
        )
        
        return False

if __name__ == "__main__":
    success = run_enhanced_crawl()
    logger.info("="*80)
    logger.info(f"í¬ë¡¤ë§ ì¢…ë£Œ - ê²°ê³¼: {'ì„±ê³µ' if success else 'ì‹¤íŒ¨'}")
    logger.info("="*80)
    sys.exit(0 if success else 1)