#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PokerScout ì‹¤ì‹œê°„ í¬ë¡¤ë§ ì‹¤í–‰ ë° ê²°ê³¼ í‘œì‹œ
online_data_collector.pyì™€ ë™ì¼í•œ ë¡œì§ìœ¼ë¡œ ì „ì²´ ì‚¬ì´íŠ¸ í¬ë¡¤ë§
+ Firebase Firestore ì—°ë™ ê¸°ëŠ¥ ì¶”ê°€ (íš¨ìœ¨ì ì¸ êµ¬ì¡°)
"""

import cloudscraper
from bs4 import BeautifulSoup
import json
from datetime import datetime, timezone
import logging
import sys
import re
import os

import firebase_admin
from firebase_admin import credentials, firestore

# Firebase Admin SDK ì´ˆê¸°í™”
try:
    key_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..', 'key', 'firebase-service-account-key.json'))
    cred = credentials.Certificate(key_path)
    if not firebase_admin._apps:
        firebase_admin.initialize_app(cred)
    db = firestore.client()
    logger = logging.getLogger(__name__)
    logger.info("Firebase Admin SDKê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)
    logger.error(f"Firebase Admin SDK ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    db = None

# ë¡œê¹… ì„¤ì • (Firebase ì´ˆê¸°í™” í›„ ì„¤ì • ë³´ì¥)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def upload_to_firestore_efficiently(data):
    """
    ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ì¸ êµ¬ì¡°ë¡œ Firestoreì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.
    - `sites` ì»¬ë ‰ì…˜: ì‚¬ì´íŠ¸ì˜ ê³ ì • ì •ë³´ ì €ì¥ (ì¤‘ë³µ ë°©ì§€)
    - `traffic_logs` í•˜ìœ„ ì»¬ë ‰ì…˜: ì‹œê°„ì— ë”°ë¥¸ íŠ¸ë˜í”½ ë°ì´í„° ì €ì¥
    """
    if not db:
        logger.error("Firestore í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ì—…ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    if not data:
        logger.warning("ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    logger.info("íš¨ìœ¨ì ì¸ êµ¬ì¡°ë¡œ Firestoreì— ë°ì´í„° ì—…ë¡œë“œ ì‹œì‘...")
    
    # Batch Writeë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ì‘ì—…ì„ í•œ ë²ˆì˜ ìš”ì²­ìœ¼ë¡œ ì²˜ë¦¬
    batch = db.batch()
    
    for site_data in data:
        site_name = site_data['site_name']
        collected_at_iso = site_data['collected_at']
        
        # 1. `sites` ì»¬ë ‰ì…˜ ì²˜ë¦¬
        site_ref = db.collection('sites').document(site_name)
        site_info = {
            'site_name': site_name,
            'category': site_data['category'],
            'last_updated_at': firestore.SERVER_TIMESTAMP # ì„œë²„ ì‹œê°„ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        }
        # set(..., merge=True)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì¡´ ë¬¸ì„œëŠ” ì—…ë°ì´íŠ¸, ì—†ëŠ” ë¬¸ì„œëŠ” ìƒì„±
        batch.set(site_ref, site_info, merge=True)

        # 2. `traffic_logs` í•˜ìœ„ ì»¬ë ‰ì…˜ ì²˜ë¦¬
        log_ref = site_ref.collection('traffic_logs').document(collected_at_iso)
        traffic_data = {
            'players_online': site_data['players_online'],
            'cash_players': site_data['cash_players'],
            'peak_24h': site_data['peak_24h'],
            'seven_day_avg': site_data['seven_day_avg'],
            # ISO 8601 ë¬¸ìì—´ì„ Firestore íƒ€ì„ìŠ¤íƒ¬í”„ ê°ì²´ë¡œ ë³€í™˜
            'collected_at': datetime.fromisoformat(collected_at_iso).replace(tzinfo=timezone.utc)
        }
        batch.set(log_ref, traffic_data)

    try:
        # Batch ì‘ì—… ì¼ê´„ ì»¤ë°‹
        batch.commit()
        logger.info(f"ì„±ê³µì ìœ¼ë¡œ {len(data)}ê°œ ì‚¬ì´íŠ¸ì˜ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ì¸ êµ¬ì¡°ë¡œ Firestoreì— ì—…ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"Firestore Batch ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(traceback.format_exc())

class LivePokerScoutCrawler:
    def __init__(self):
        self.scraper = cloudscraper.create_scraper(
            browser={'browser': 'chrome', 'platform': 'linux', 'mobile': False}
        )
        self.gg_poker_sites = ['GGNetwork', 'GGPoker ON', 'GG Poker', 'GGPoker']
        
    def crawl_pokerscout_data(self):
        logger.info("PokerScout ì‹¤ì‹œê°„ í¬ë¡¤ë§ ì‹œì‘...")
        try:
            response = self.scraper.get('https://www.pokerscout.com', timeout=30)
            response.raise_for_status()
            logger.info(f"ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}")
            
            soup = BeautifulSoup(response.content, 'html.parser')
            table = soup.find('table', {'class': 'rankTable'})
            if not table:
                logger.error("PokerScout í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            logger.info("rankTable ë°œê²¬!")
            collected_data = []
            rows = table.find_all('tr')[1:]
            logger.info(f"ë°œê²¬ëœ í–‰ ìˆ˜: {len(rows)}")
            
            for i, row in enumerate(rows):
                try:
                    if 'cus_top_traffic_coin' in row.get('class', []):
                        continue
                    brand_title = row.find('span', {'class': 'brand-title'})
                    if not brand_title:
                        continue
                    site_name = brand_title.get_text(strip=True)
                    if not site_name or len(site_name) < 2:
                        continue
                    
                    players_online, cash_players, peak_24h, seven_day_avg = 0, 0, 0, 0
                    
                    online_td = row.find('td', {'id': 'online'})
                    if online_td and (span := online_td.find('span')) and (text := span.get_text(strip=True).replace(',', '')) and text.isdigit():
                        players_online = int(text)
                    
                    cash_td = row.find('td', {'id': 'cash'})
                    if cash_td and (text := cash_td.get_text(strip=True).replace(',', '')) and text.isdigit():
                        cash_players = int(text)

                    peak_td = row.find('td', {'id': 'peak'})
                    if peak_td and (span := peak_td.find('span')) and (text := span.get_text(strip=True).replace(',', '')) and text.isdigit():
                        peak_24h = int(text)

                    avg_td = row.find('td', {'id': 'avg'})
                    if avg_td and (span := avg_td.find('span')) and (text := span.get_text(strip=True).replace(',', '')) and text.isdigit():
                        seven_day_avg = int(text)

                    if players_online == 0 and cash_players == 0 and peak_24h == 0:
                        continue
                    
                    site_name = re.sub(r'[^\w\s\-\(\)\.&]', '', site_name).strip()
                    category = 'GG_POKER' if site_name in self.gg_poker_sites else 'COMPETITOR'
                    
                    collected_data.append({
                        'site_name': site_name,
                        'category': category,
                        'players_online': players_online,
                        'cash_players': cash_players,
                        'peak_24h': peak_24h,
                        'seven_day_avg': seven_day_avg,
                        'collected_at': datetime.now(timezone.utc).isoformat()
                    })
                except Exception as e:
                    logger.error(f"í–‰ {i+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue
            
            logger.info(f"í¬ë¡¤ë§ ì™„ë£Œ: {len(collected_data)}ê°œ ì‚¬ì´íŠ¸ ìˆ˜ì§‘")
            return collected_data
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return []
    
    def analyze_and_save(self, data):
        if not data:
            logger.error("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return
        
        logger.info("\n" + "="*60)
        logger.info("í¬ë¡¤ë§ ê²°ê³¼ ë¶„ì„")
        logger.info("="*60)
        
        total_sites = len(data)
        total_players = sum(site['players_online'] for site in data)
        logger.info(f"ì´ ì‚¬ì´íŠ¸ ìˆ˜: {total_sites}ê°œ, ì´ ì˜¨ë¼ì¸ í”Œë ˆì´ì–´: {total_players:,}ëª…")
        
        # ë¡œì»¬ JSON íŒŒì¼ ì €ì¥ (ë°±ì—… ë° ë””ë²„ê¹…ìš©)
        filename = f"live_crawling_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ ê²°ê³¼ê°€ ë¡œì»¬ ë°±ì—… íŒŒì¼({filename})ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # íš¨ìœ¨ì ì¸ êµ¬ì¡°ë¡œ Firestoreì— ì—…ë¡œë“œ
        upload_to_firestore_efficiently(data)
        
        return data

if __name__ == '__main__':
    crawler = LivePokerScoutCrawler()
    crawled_data = crawler.crawl_pokerscout_data()
    if crawled_data:
        crawler.analyze_and_save(crawled_data)
