name: Daily Poker Data Crawl

on:
  schedule:
    # 매일 UTC 18:00 (한국시간 오전 3시)에 실행
    - cron: '0 18 * * *'
  workflow_dispatch: # 수동 실행 가능

jobs:
  crawl:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        cd backend
        pip install -r requirements.txt
        # Ensure cloudscraper is installed
        pip install cloudscraper beautifulsoup4 lxml
    
    - name: Create Firebase key from secret
      env:
        FIREBASE_KEY: ${{ secrets.FIREBASE_SERVICE_ACCOUNT_KEY }}
      run: |
        mkdir -p backend/key
        echo "$FIREBASE_KEY" > backend/key/firebase-service-account-key.json
    
    - name: Run crawler
      run: |
        cd backend
        python -c "
        import sys
        import os
        import traceback
        
        # Add backend to path
        sys.path.insert(0, os.getcwd())
        
        print('=== GitHub Actions Crawling Start ===')
        print(f'Current directory: {os.getcwd()}')
        print(f'Python path: {sys.path[:3]}')
        
        try:
            # Import check
            print('Importing crawler modules...')
            from app.services.poker_crawler import LivePokerScoutCrawler, upload_to_firestore_efficiently
            print('Import successful!')
            
            # Create crawler instance
            print('Creating crawler instance...')
            crawler = LivePokerScoutCrawler()
            
            # Crawl data
            print('Starting crawl...')
            crawled_data = crawler.crawl_pokerscout_data()
            
            if crawled_data:
                print(f'Crawl success: {len(crawled_data)} sites found')
                
                # Upload to Firebase
                print('Uploading to Firebase...')
                upload_to_firestore_efficiently(crawled_data)
                print('Firebase upload complete!')
            else:
                print('ERROR: No data crawled')
                sys.exit(1)
                
        except ImportError as e:
            print(f'Import Error: {e}')
            print('Directory contents:')
            for root, dirs, files in os.walk('.'):
                level = root.replace('.', '').count(os.sep)
                indent = ' ' * 2 * level
                print(f'{indent}{os.path.basename(root)}/')
                subindent = ' ' * 2 * (level + 1)
                for file in files[:5]:  # Limit to 5 files per dir
                    print(f'{subindent}{file}')
            sys.exit(1)
        except Exception as e:
            print(f'Error occurred: {e}')
            traceback.print_exc()
            sys.exit(1)
            
        print('=== Crawling Complete ===')
        "
    
    - name: Clean up
      if: always()
      run: |
        rm -rf backend/key